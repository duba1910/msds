Daily_Production <- read.table( "Daily_Production.txt", header = TRUE)
Daily.Production <- read.table( "Daily_Production.txt", header = TRUE)
#### Probability Distributions ###
# Example of Binomial Probability Distribution
table.dist.binomial(n = 2, p = 0.2)
require(lolcat)
Daily.Production <- read.table( "Daily_Production.txt", header = TRUE)
# Example of Binomial Probability Distribution
table.dist.binomial(n = 2, p = 0.2)
# Barplot of Binomal Probability Distribution
n<-2
P<-0.2
data<-dbinom(x = 0:n, size = n, prob = P)
names(data)<-0:n
barplot(data, xlab = "# of Defectives", ylab = "P(D)", ylim = c(0,1))
# Probability Distribtuion for Discrete Random Variable
(freqdistdp<-round.object(frequency.dist.grouped(Daily.Production$V1),3))
require(lolcat)
Daily.Production <- read.table( "Daily_Production.txt", header = TRUE)
#### Probability Distributions ###
# Example of Binomial Probability Distribution
table.dist.binomial(n = 2, p = 0.2)
n<-2
P<-0.2
data<-dbinom(x = 0:n, size = n, prob = P)
names(data)<-0:n
barplot(data, xlab = "# of Defectives", ylab = "P(D)", ylim = c(0,1))
# Probability Distribution for Discrete Random Variable
(freqdistdp<-round.object(frequency.dist.grouped(Daily.Production$V1),3))
View(Daily.Production)
View(Daily_Production)
View(Daily.Production)
require(lolcat)
Daily.Production <- read.table( "Daily_Production.txt", header = FALSE)
#### Probability Distributions ###
# Example of Binomial Probability Distribution
table.dist.binomial(n = 2, p = 0.2)
# Barplot of Binomal Probability Distribution
n<-2
P<-0.2
data<-dbinom(x = 0:n, size = n, prob = P)
names(data)<-0:n
barplot(data, xlab = "# of Defectives", ylab = "P(D)", ylim = c(0,1))
# Probability Distribution for Discrete Random Variable
(freqdistdp<-round.object(frequency.dist.grouped(Daily.Production$V1),3))
(probdistdp<-freqdistdp[,c("min","freq","rel.freq")])
colnames(probdistdp)<-c("Daily Production", "#of Days", "P(DP)")
View(probdistdp)
# Probability Distribution (Histogram)
hist.grouped(Daily.Production$V1, freq = F, anchor.value=50, ylim=c(0,0.20))
# Expected Value of a Discrete Random Variable
x<-probdistdp$`Daily Production`
y<-probdistdp$`P(DP)`
weighted.mean(x,y)
# Alternatively
mean(Daily.Production$V1)
# Probability Distribution for Discrete Random Variable
(freqdistdp<-round.object(frequency.dist.grouped(Daily.Production$V1),3))
(probdistdp<-freqdistdp[,c("min","freq","rel.freq")])
colnames(probdistdp)<-c("Daily Production", "#of Days", "P(DP)")
View(probdistdp)
# Probability Distribution (Histogram)
hist.grouped(Daily.Production$V1, freq = F, anchor.value=50, ylim=c(0,0.20))
# Expected Value of a Discrete Random Variable
x<-probdistdp$`Daily Production`
y<-probdistdp$`P(DP)`
weighted.mean(x,y)
# Alternatively
mean(Daily.Production$V1)
p<-0.80
q<-0.20
r<-45
n<-50
factorial(n)/(factorial(r)*factorial(n-r))*(p^r)*(q^(n-r))
# Binomial Example - dbinom
dbinom(x = 45, size = 50, prob = 0.8)
# Barplot of Binomal Probability Distribution
n<-50
P<-0.8
data<-dbinom(x = 26:n, size = n, prob = P)
names(data)<-26:n
barplot(data, xlab = "# of Good Parts", ylab = "P(G)", ylim = c(0,0.15))
round.object(table.dist.binomial(n = 50, p = 0.80),5)
# Barplot of Binomal Probability Distribution
n<-50
P<-0.8
data<-dbinom(x = 26:n, size = n, prob = P)
names(data)<-26:n
barplot(data, xlab = "# of Good Parts", ylab = "P(G)", ylim = c(0,0.15))
cols <- rep("grey", n + 1)
cols[20:25] <- "red"
barplot(data, col=cols,xlab = "# of Good Parts", ylab = "P(G)", ylim = c(0,0.15))
pbinom(q = 44, size = 50, prob = 0.80, lower.tail = F)
round.object(table.dist.poisson(lambda = 25),5)
round.object(table.dist.poisson(lambda = 25)[7:51,],5)
# Barplot of Poisson Probability Distribution
lambda<-25
X<-10
data<-dpois(x = 6:50, lambda = lambda)
names(data)<-6:50
barplot(data, xlab = "Parts per Hour", ylab = "P(X)", ylim = c(0, 0.10))
# Poisson Probability of 10 or fewer
ppois(q = 10, lambda = 25, lower.tail = T)
# Poisson Probability of at least 20, but no more than 30?
data<-dpois(x = 6:50, lambda = lambda)
names(data)<-6:50
barplot(data, xlab = "Parts per Hour", ylab = "P(X)", ylim = c(0, 0.10))
cols <- rep("grey", n + 1)
cols[15:25] <- "red"
barplot(data, col=cols, xlab = "Parts per Hour", ylab = "P(X)", ylim = c(0, 0.10))
(ft20<-ppois(q = 19, lambda = 25, lower.tail = T))
(ft30<-ppois(q = 30, lambda = 25, lower.tail = T))
ft30-ft20
# Poisson distribution testing
poisdist<-rpois(n = 100, lambda = 25)
poisson.dist.test(poisdist)
summary.continuous(normdata)
normdata<-rnorm(n = 24, mean = 10, sd = 2)
summary.continuous(normdata)
anderson.darling.normality.test(normdata)
summary.continuous(normdata)
pexp(q = 60, rate = 1/100, lower.tail = F)
x=seq(0,800,length=200)
y=dexp(x, rate = 1/100)
plot(x,y,type="l")
# Shade the upper tail area
x=seq(60,800,length=100)
y=dexp(x, rate = 1/100)
polygon(c(60,x,800),c(0,y,0),col="red")
pexp(q = (20-5),rate = 1/(50-5), lower.tail = T)
pexp.low(q = 20, low = 5, mean = 50, lower.tail = T)
Daily.Production <- read.table( "airline.txt", header = TRUE)
airline <- read.table( "airline.txt", header = TRUE)
require(lolcat)
airline <- read.table( "airline.txt", header = TRUE)
require(lolcat)
# Read the file into a variable
lines <- readLines("airline.dat")
# Initialize an empty list to store fixed lines
fixed_lines <- list()
# Iterate through each line and fix lines with fewer than 5 elements
for (line in lines) {
elements <- unlist(strsplit(line, "\t"))
if (length(elements) < 5) {
elements <- c(elements, rep("0", 5 - length(elements)))
}
fixed_lines <- c(fixed_lines, paste(elements, collapse = "\t"))
}
# Convert fixed lines to a data frame
data_matrix <- do.call(rbind, lapply(fixed_lines, function(x) strsplit(x, "\t")[[1]]))
colnames(data_matrix) <- data_matrix[1, ]
data_matrix <- data_matrix[-1, ]
airline_df <- as.data.frame(data_matrix, stringsAsFactors = FALSE)
# Convert appropriate columns to numeric
airline <- transform(airline_df,
Price = as.numeric(Price),
Number_of_Hours_Late = as.numeric(Number_of_Hours_Late),
Number_Cancelled_per_Day_per_500 = as.numeric(Number_Cancelled_per_Day_per_500),
Bag_Delivery_Time = as.numeric(Bag_Delivery_Time),
Number_Lost_Bags = as.numeric(Number_Lost_Bags))
View(airline_df)
lt930 <- subset(airline, Price < 930)
37/5400
summary.continuous(airline$Price)
shapiro.wilk.normality.test(airline$Price)
normdata<-rnorm(n = 24, mean = 10, sd = 2)
summary.continuous(airline$Price)
(normout<-dagostino.normality.omnibus.test(airline$Price))#normal = p>=.05
(skkupvals<-c(normout$estimate[6], normout$estimate[12]))#normal = p>=.05
pnorm(q = 930, mean = mean(airline$Price), sd = sd(airline$Price), lower.tail = T)
View(airline)
pnorm(q = 2, mean = mean(airline$Number_Cancelled_per_Day_per_500), sd = sd(airline$Number_Cancelled_per_Day_per_500), lower.tail = F)
require(lolcat)
airline <- read.table("airline.dat", header = FALSE, skip = 1)
mean_cancelled_flights <- mean(airline$Number_Cancelled_per_Day_per_500, na.rm = TRUE)
lambda_20_flights <- mean_cancelled_flights * (20 / 500)
1 - ppois(1, lambda_20_flights)
1 - ppois(1, lambda_20_flights) - ppois(0, lambda_20_flights)
ppois(1, lambda_20_flights)*2
lambda_20_flights <- mean_cancelled_flights * (20 / 450)
1 - ppois(1, lambda_20_flights)
lambda_20_flights <- mean_cancelled_flights * (1 / 500)
one_flight_cancelled <- ppois(1, lambda_20_flights)
one_flight_cancelled*2
lambda_one_flights <- mean_cancelled_flights * (1 / 500)
one_flight_cancelled <- ppois(0, lambda_one_flights)
one_flight_cancelled*2
lambda_one_flights <- mean_cancelled_flights * (1 / 500)
one_flight_cancelled <- ppois(0, lambda_one_flights, lower.tail = FALSE)
one_flight_cancelled*2
rate_parameter <- 1 / mean(airline$Bag_Delivery_Time, na.rm = TRUE)
prob_wait_40_or_more <- pexp(40, rate = rate_parameter, lower.tail = FALSE)
prob_wait_40_or_more * 100
mean_lost_bags <- mean(airline$Number_Lost_Bags, na.rm = TRUE)
prob_exactly_5_lost_bags <- dpois(5, lambda = mean_lost_bags)
print(prob_exactly_5_lost_bags)
mean_cancellations_per_500 <- mean(airline$Number_Cancelled_per_Day_per_500)
# Calculate the probability of a single flight being canceled
p_cancel <- mean_cancellations_per_500 / 500
# Number of flights in the sample
n_flights <- 20
# Calculate the probability of having at least 2 canceled flights out of 20
prob_at_least_2_cancellations <- 1 - pbinom(1, n_flights, p_cancel)
prob_at_least_2_cancellations
# Calculate the probability of a single flight being canceled
p_cancel <- mean_cancellations_per_500 / 450
# Number of flights in the sample
n_flights <- 20
# Calculate the probability of having at least 2 canceled flights out of 20
prob_at_least_2_cancellations <- 1 - pbinom(1, n_flights, p_cancel)
prob_at_least_2_cancellations
mean_cancellations_per_500 <- mean(airline$Number_Cancelled_per_Day_per_500)
# Calculate the probability of a single flight being canceled
p_cancel <- mean_cancellations_per_500
# Number of flights in the sample
n_flights <- 20
# Calculate the probability of having at least 2 canceled flights out of 20
prob_at_least_2_cancellations <- 1 - dbinom(1, n_flights, p_cancel)
prob_at_least_2_cancellations
8/480
round(8/480, 4)
pbinom(q = 4, size = 400, prob = 0.02, lower.tail = T)
ppois(q = 50, lambda = 65, lower.tail = F)
#Create output variables
(lower<-pnorm(q = 154, mean = 153, sd = 10, lower.tail = T))
(upper<-pnorm(q = 164,mean = 153, sd = 10,, lower.tail = F))
#Add together for total area under the normal curve
(total=lower+upper)
(totalpercent=total*100)
round.object(totalpercent,2)
#Add together for total area under the normal curve
(total=lower+upper)
(totalpercent=total*100)
round.object(totalpercent,4)
(total=lower+upper)
round.object(total,4)
pexp(q = 10000, rate = 1/40000, lower.tail = T)
round(pexp(q = 10000, rate = 1/40000, lower.tail = T), 4)
# Assume that a batch of incoming parts has a defective rate of 2%  or 0.02
# (A part is either defective or not defective).
# A sample of 400 parts is drawn and inspected.
# What is the probability of finding 4 or fewer defectives?
pbinom(q = 4, size = 400, prob = 0.02, lower.tail = T)
# A warehouse team member has been monitoring the daily shipment rate of automobile accessories.
# On average, the number of pallets per day that have been shipped is 65 (lambda).
# The shipment rate has been statistically stable and has been shown to be representative of a Poisson function.
# The warehouse manager recently asked what the probability of shipping 50 pallets or more in a day was.
ppois(q = 49, lambda = 65, lower.tail = F)
# A normally distributed process has typically run at a mean of 153 with a sd of 10.
# The specifications for the part are 159 ± 5 (meaning the upper specification limit is 164
# and the lower specification limit is 154).
# What is the probability that a single part selected at random from a standard lot will be out of specification?
lower<-pnorm(q = 154, mean = 153, sd = 10, lower.tail = T)
source("C:/Users/dusti/OneDrive/Documents/GitHub/msds/Data Science Methods for Quality Improvement/5704 Managing, Describing, and Analyzing Data/m_3/quiz_probability_and_probability_distributions.R")
ppois(q = 49, lambda = 65, lower.tail = F)
require(lolcat)
airline <- read.table( "airline.dat", header = FALSE)
airline <- read.table( "airline.dat", header = TRUE)
price_data <- airline$Price[0:5400]
price_data <- airline$Price[0:5400]
(normout<-dagostino.normality.omnibus.test(price_data))#normal = p>=.05
(skkupvals<-c(normout$estimate[6], normout$estimate[12]))#normal = p>=.05
# For the data associated with Ticket Prices: What percentage of the population of the passengers
# flying during the period represented by these data paid less than $930.00 for their ticket?
pnorm(q = 930, mean = mean(price_data), sd = sd(price_data), lower.tail = T)
cancel_data = airline$Number_Cancelled_per_Day_per_500[0:450]
mean_cancel = mean(cancel_data)
prob_cancel = mean_cancel/500
pbinom(q = 2 , size = 20 , prob = prob_cancel, lower.tail = F)
pbinom(q = 3 , size = 20 , prob = prob_cancel, lower.tail = F)
pbinom(q = 1 , size = 20 , prob = prob_cancel, lower.tail = F)
1- pbinom(q = 2 , size = 20 , prob = prob_cancel, lower.tail = T)
1- pbinom(q = 18 , size = 20 , prob = prob_cancel, lower.tail = T)
1- pbinom(q = 18 , size = 20 , prob = prob_cancel, lower.tail = F)
1- pbinom(q = 2 , size = 20 , prob = prob_cancel, lower.tail = F)
1- pbinom(q = 1 , size = 20 , prob = prob_cancel, lower.tail = F)
1- pbinom(q = 1 , size = 20 , prob = prob_cancel, lower.tail = T)
mean_cancel*2
prob_cancel*2
prob_cancel^2
bag_data = airline$Bag_Delivery_Time[0:75]
mean_bag_time = mean(bag_data)
ans1 = pexp(q = 40, rate = 1/mean_bag_time, lower.tail = T)
ans1 * 100
bag_data = airline$Bag_Delivery_Time[0:75]
mean_bag_time = mean(bag_data)
ans1 = pexp(q = 40, rate = 1/mean_bag_time, lower.tail = T)
pexp(q = 40, rate = 1/mean_bag_time, lower.tail = T)
pexp(q = 40, rate = 1/mean_bag_time, lower.tail = F)
1-pexp(q = 40, rate = 1/mean_bag_time, lower.tail = F)
pexp(q = 15, rate = 1/mean_bag_time, lower.tail = F)+pexp(q = 25, rate = 1/mean_bag_time, lower.tail = F)
bag_data = airline$Bag_Delivery_Time[0:75]
num_passengers_waited_25_more <- sum(bag_data >= 25)
# Calculate the total number of passengers in the sample
total_passengers <- length(bag_data)
# Calculate the percentage of passengers who waited at least 25 more minutes
percentage_waited_25_more <- (num_passengers_waited_25_more / total_passengers) * 100
percentage_waited_25_more
pexp(q = 25, rate = 1/mean_bag_time, lower.tail = F)
shapetest.exp.epps.pulley.1986(bag_data)
expdata<-rexp(n = 75, rate = 1/mean_bag_time,)
expdata<-rexp(n = 75, rate = 1/mean_bag_time)
shapiro.wilk.exponentiality.test(expdata)  #exp = p>=.05
lost_bag_data = airline$Bag_Delivery_Time[0:50]
mean_lost_bags <- mean(airline$Number_Lost_Bags, na.rm = TRUE)
dpois(5, lambda = mean_lost_bags)
dpois(5, lambda = mean_lost_bags, lower.tail = true)
dpois(4, lambda = mean_lost_bags)
lost_bag_data = airline$Bag_Delivery_Time[0:50]
mean_lost_bags <- mean(lost_bag_data, na.rm = TRUE)
dpois(5, lambda = mean_lost_bags)
ppois(5, lambda = mean_lost_bags)
ppois(5, lambda = mean_lost_bags, lower.tail = TRUE)
ppois(9, lambda = mean_lost_bags, lower.tail = FALSE)
price_data <- airline$Price[0:5400]
(normout<-dagostino.normality.omnibus.test(price_data))#normal = p>=.05
(skkupvals<-c(normout$estimate[6], normout$estimate[12]))#normal = p>=.05
# For the data associated with Ticket Prices: What percentage of the population of the passengers
# flying during the period represented by these data paid less than $930.00 for their ticket?
pnorm(q = 930, mean = mean(price_data), sd = sd(price_data), lower.tail = T)
cancel_data = airline$Number_Cancelled_per_Day_per_500[0:450]
mean_cancel = mean(cancel_data)
prob_cancel = mean_cancel/500
1- pbinom(q = 1 , size = 20 , prob = prob_cancel, lower.tail = T)
prob_cancel^2
bag_data = airline$Bag_Delivery_Time[0:75]
mean_bag_time = mean(bag_data)
pexp(q = 25, rate = 1/mean_bag_time, lower.tail = F)
lost_bag_data = airline$Bag_Delivery_Time[0:50]
mean_lost_bags <- mean(lost_bag_data, na.rm = TRUE)
dpois(5, lambda = mean_lost_bags)
ppois(5, lambda = mean_lost_bags, lower.tail = TRUE)
ppois(9, lambda = mean_lost_bags, lower.tail = FALSE)
bag_data = airline$Bag_Delivery_Time[0:75]
mean_bag_time = mean(bag_data)
pexp(q = 40, rate = 1/mean_bag_time, lower.tail = F)
mean_bag_time = mean(bag_data)
dpois(5, lambda = mean_lost_bags)
pexp(q = 40-25, rate = 1/(mean_bag_time-25), lower.tail = F)
pexp(q = 40-15, rate = 1/(mean_bag_time-15), lower.tail = F)
pexp(q = 40-15, rate = 1/(mean_bag_time-15), lower.tail = T)
pexp(q = 25-15, rate = 1/(mean_bag_time-15), lower.tail = T)
pexp(q = 25-15, rate = 1/(mean_bag_time-15), lower.tail = F)
pexp.low(q = 40, low = 15, mean = mean_bag_time, lower.tail = T)
#### Sampling Error ####
# Create four random samples of size n=30 with Mu = 100, sigma = 10
d1<-rnorm(n = 30,mean = 100, sd = 10)
d2<-rnorm(n = 30,mean = 100, sd = 10)
d3<-rnorm(n = 30,mean = 100, sd = 10)
d4<-rnorm(n = 30,mean = 100, sd = 10)
# Create a dataframe of all variables
normdata<-data.frame(d1,d2,d3,d4)
View(normdata)
summary.all.variables(normdata, stat.sd=T)
par(mfrow=c(2,2))
hist.grouped(normdata$d1)
hist.grouped(normdata$d2)
hist.grouped(normdata$d3)
hist.grouped(normdata$d4)
par(mfrow=c(1,1))
randoexp<-rexp(n = 100, rate = 1/10)
hist.grouped(randoexp)
#### Random Sampling Distributions ####
# Random Sampling Distribution Simulation
nqtr<-function(x,d){noquote(t(round.object(x, d)))}
# First, set seed so we will get the same results
set.seed(133)
# Create a distribution with mean of 10 and standard deviation of 2
pop<-rnorm(n = 50000, mean = 10, sd = 2)
# Create a histogram of the population distribution
hist.grouped(pop, anchor.value = 0)
# Calculate the descriptive statistics of the population distribution
nqtr(summary.continuous(pop, stat.sd=T),5)
# Next, create / simulate a random sampling distribution
# Set the sample size equal to 4
n<-4
# Create the number of repetitions
# (number of times we will take a sample of size 4)
reps<-5000
# Take the random samples from a population with a mean of 10 and
# standard deviation of 2
samples <- replicate(reps, rnorm(n, mean = 10, sd = 2))
View(samples)
# Calculate the averages of each sample of 4
sampleavgs <- colMeans(samples)
# Calculate the descriptive statistics of the RSD
nqtr(summary.continuous(sampleavgs, stat.sd=T),5)
# Create a 1 x 2 matrix
dev.off() # turns off the default
layout(matrix(1:2, nrow=2))
hist.grouped(pop, xlim=c(0,22), xaxt='n', width.consider = 1)
# Create a 1 x 2 matrix
dev.off() # turns off the default
layout(matrix(1:2, nrow=2))
# Create both histograms
hist.grouped(pop, xlim=c(0,22), xaxt='n', width.consider = 1)
# Create both histograms
hist.grouped(pop, xlim=c(0,22), xaxt='n', width.consider = 1)
# Create both histograms
hist.grouped(pop, xlim=c(0,22), xaxt='n', width.consider = 1)
axis(side = 1, at = seq(0,22,2), labels = seq(0,22,2))
hist.grouped(sampleavgs, xlim=c(0,22), xaxt='n', width.consider = 1)
axis(side = 1, at = seq(0,22,2), labels = seq(0,22,2))
set.seed(101)
popexp<-rexp(n = 50000, rate = 1/10)
# Create a histogram of the population distribution
hist.grouped(popexp)
set.seed(102)
# Set the sample size equal to 100
n<-100
# Create the number of repetitions
# (number of times we will take a sample of size 100)
reps<-1000
# Take the random samples from an exponential population with a
# mean of 10 (lambda = 1/10)
samplesexp <- replicate(reps, rexp(n, rate = 1/10))
View(samplesexp)
# Calculate the averages of each sample of 100
sampleavgsexp <- colMeans(samplesexp)
# Calculate the descriptive statistics of the RSD of the means
nqtr(summary.continuous(sampleavgsexp, stat.sd=T),5)
# Create a histogram for the RSD of the means from the
# exponential population
hist.grouped(sampleavgsexp)
# Create a 1 x 2 matrix
dev.off() # turns off the default
layout(matrix(1:2, nrow=2))
# Create both histograms
hist.grouped(popexp, xlim=c(0,50), xaxt='n', width.consider = 2)
axis(side = 1, at = seq(0,50,5), labels = seq(0,50,5))
hist.grouped(sampleavgsexp, xlim=c(0,50), xaxt='n', width.consider = 2)
axis(side = 1, at = seq(0,50,5), labels = seq(0,50,5))
mu1<-1.325
sigma1<-0.045
n1<-25
xbar1<-1.433
stderror1<-sigma1/sqrt(n1) #Standard Error= SD/sqrt(n)
#zcore = x bar - mu / standard error
# Calculate the area under the normal curve using the pnorm function
pnorm(q = xbar1, mean = mu1,sd = stderror1, lower.tail = F)
# Create the normal curve
x=seq(1.28,1.45,length=200)
y=dnorm(x,mean=mu1,sd=stderror1)
plot(x,y,type="l")
# Indicate the location of the xbar of 1.433
abline(v=1.433)
mu2<-50
sigma2<-14.4
n2<-16
xbar2<-55
stderror2<-sigma2/sqrt(n2)
stderror2
# Calculate the area under the normal curve using the pnorm function
pnorm(q = xbar2, mean = mu2,sd = stderror2, lower.tail = F)
# Create the normal curve
x=seq(30,70,length=200)
y=dnorm(x,mean=mu2,sd=stderror2)
plot(x,y,type="l")
x=seq(55,70,length=100)
y=dnorm(x,mean=mu2,sd=stderror2)
polygon(c(55,x,70),c(0,y,0),col="red")
# Indicate the location of the xbar of 55
abline(v=55)
View(Point_Estimates)
Point_Estimates <- read.table( "Point_Estimates.txt", header = FALSE)
# Point Estimates
Point_Estimates <- read.table( "Point_Estimates.dat", header = FALSE)
